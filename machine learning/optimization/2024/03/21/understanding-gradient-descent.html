<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Understanding Gradient Descent: The Backbone of Modern Machine Learning | Jianting Feng</title> <meta name="author" content="Jianting Feng"> <meta name="description" content="Jianting Feng - Postgraduate Student in Statistics at The Chinese University of Hong Kong "> <meta name="keywords" content="Statistics, Machine Learning, Optimization, Research"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%8A&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jiantingfeng.github.io/machine%20learning/optimization/2024/03/21/understanding-gradient-descent.html"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Jianting Feng</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/photos/">Photos</a> </li> <li class="nav-item "> <a class="nav-link" href="/code/">Software &amp; Code</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding Gradient Descent: The Backbone of Modern Machine Learning</h1> <p class="post-meta">March 21, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/gradient-descent"> <i class="fas fa-hashtag fa-sm"></i> gradient-descent</a>   <a href="/blog/tag/optimization"> <i class="fas fa-hashtag fa-sm"></i> optimization</a>   <a href="/blog/tag/machine-learning"> <i class="fas fa-hashtag fa-sm"></i> machine-learning</a>     ·   <a href="/blog/category/machine-learning"> <i class="fas fa-tag fa-sm"></i> Machine Learning</a>   <a href="/blog/category/optimization"> <i class="fas fa-tag fa-sm"></i> Optimization</a>   </p> </header> <article class="post-content"> <p>Gradient descent is one of the most fundamental optimization algorithms in machine learning. It’s the workhorse behind training neural networks, linear regression models, and many other machine learning algorithms. In this post, we’ll explore what gradient descent is, how it works, and why it’s so important in modern machine learning.</p> <h2 id="table-of-contents">Table of Contents</h2> <ul> <li><a href="#table-of-contents">Table of Contents</a></li> <li><a href="#what-is-gradient-descent">What is Gradient Descent?</a></li> <li><a href="#the-mathematics-behind-gradient-descent">The Mathematics Behind Gradient Descent</a></li> <li> <a href="#types-of-gradient-descent">Types of Gradient Descent</a> <ul> <li><a href="#1-batch-gradient-descent">1. Batch Gradient Descent</a></li> <li><a href="#2-stochastic-gradient-descent-sgd">2. Stochastic Gradient Descent (SGD)</a></li> <li><a href="#3-mini-batch-gradient-descent">3. Mini-batch Gradient Descent</a></li> </ul> </li> <li><a href="#practical-implementation">Practical Implementation</a></li> <li> <a href="#key-considerations">Key Considerations</a> <ul> <li><a href="#learning-rate-selection">Learning Rate Selection</a></li> <li><a href="#feature-scaling">Feature Scaling</a></li> <li><a href="#convergence-criteria">Convergence Criteria</a></li> </ul> </li> <li><a href="#conclusion">Conclusion</a></li> <li><a href="#references">References</a></li> </ul> <h2 id="what-is-gradient-descent">What is Gradient Descent?</h2> <p>At its core, gradient descent is an iterative optimization algorithm used to minimize a function. In machine learning, this function is typically a loss function that measures how well our model is performing. The goal is to find the parameters that minimize this loss function.</p> <blockquote> <p><strong>Key Idea</strong>: Gradient descent works by iteratively moving in the direction of steepest descent, guided by the negative gradient of the function.</p> </blockquote> <h2 id="the-mathematics-behind-gradient-descent">The Mathematics Behind Gradient Descent</h2> <p>The basic idea is simple:</p> <ol> <li>Start at a random point in the parameter space</li> <li>Calculate the gradient (derivative) of the loss function at that point</li> <li>Move in the direction opposite to the gradient</li> <li>Repeat until convergence</li> </ol> <p>The update rule can be written as:</p> \[\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t)\] <p>where:</p> <ul> <li>$\theta_t$ represents the parameters at step t</li> <li>$\eta$ is the learning rate</li> <li>$\nabla J(\theta_t)$ is the gradient of the loss function</li> </ul> <h2 id="types-of-gradient-descent">Types of Gradient Descent</h2> <p>There are three main variants of gradient descent, each with its own advantages and trade-offs:</p> <h3 id="1-batch-gradient-descent">1. Batch Gradient Descent</h3> <ul> <li>Uses the entire training dataset to compute the gradient at each step</li> <li>Provides stable convergence but can be computationally expensive</li> <li>Best for small to medium-sized datasets</li> </ul> <h3 id="2-stochastic-gradient-descent-sgd">2. Stochastic Gradient Descent (SGD)</h3> <ul> <li>Uses a single training example at a time</li> <li>Faster updates but more noisy convergence</li> <li>Good for large datasets and online learning</li> </ul> <h3 id="3-mini-batch-gradient-descent">3. Mini-batch Gradient Descent</h3> <ul> <li>Uses a small subset of the training data</li> <li>Balances computational efficiency and convergence stability</li> <li>Most commonly used in practice</li> </ul> <h2 id="practical-implementation">Practical Implementation</h2> <p>Here’s a clean implementation of gradient descent for linear regression in Python:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Perform gradient descent optimization for linear regression.
    
    Parameters:
    -----------
    X : numpy.ndarray
        Feature matrix of shape (m, n)
    y : numpy.ndarray
        Target vector of shape (m,)
    learning_rate : float
        Step size for each iteration
    num_iterations : int
        Number of iterations to perform
        
    Returns:
    --------
    numpy.ndarray
        Optimized parameters
    </span><span class="sh">"""</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="c1"># Calculate gradient
</span>        <span class="n">gradient</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
        <span class="c1"># Update parameters
</span>        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span>
        
    <span class="k">return</span> <span class="n">theta</span>
</code></pre></div></div> <h2 id="key-considerations">Key Considerations</h2> <p>When implementing gradient descent, several factors need to be carefully considered:</p> <h3 id="learning-rate-selection">Learning Rate Selection</h3> <ul> <li>Too large: Algorithm might diverge</li> <li>Too small: Slow convergence</li> <li>Solution: Use learning rate scheduling or adaptive methods</li> </ul> <h3 id="feature-scaling">Feature Scaling</h3> <ul> <li>Scale features to similar ranges</li> <li>Improves convergence stability</li> <li>Common methods: Standardization, Normalization</li> </ul> <h3 id="convergence-criteria">Convergence Criteria</h3> <ul> <li>Maximum number of iterations</li> <li>Loss function change threshold</li> <li>Validation set performance</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Gradient descent is a powerful and versatile optimization algorithm that forms the foundation of many machine learning models. Understanding its mechanics and variations is crucial for anyone working in the field of machine learning and optimization.</p> <p>In future posts, we’ll explore more advanced topics like:</p> <ul> <li>Adaptive learning rates</li> <li>Momentum and Nesterov acceleration</li> <li>Second-order optimization methods</li> </ul> <p>Stay tuned for more insights into the fascinating world of machine learning optimization!</p> <h2 id="references">References</h2> <ol> <li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.</li> <li>Bottou, L. (2012). Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade.</li> </ol> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Jianting Feng. © 2024 Jianting Feng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: April 20, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",processEscapes:!0,inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-DLKPXN9214"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-DLKPXN9214");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>